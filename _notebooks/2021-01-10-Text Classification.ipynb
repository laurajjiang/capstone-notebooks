{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.6 64-bit",
            "metadata": {
                "interpreter": {
                    "hash": "1ef63b898a85616871e078e6511258e27adff349acc8e961d6c4e5343555c5a7"
                }
            }
        },
        "language_info": {
            "name": "python",
            "version": "3.8.6-final",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "state": {
                    "04f327f8729c4dfb8064560e564bc0c6": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "1.5.0",
                        "model_name": "ButtonStyleModel",
                        "state": {}
                    },
                    "527b16c6484f41a8bbea8b6f91ee4137": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "1.5.0",
                        "model_name": "ButtonStyleModel",
                        "state": {}
                    },
                    "55400c05e9674a448f4e1fca689799eb": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "1.5.0",
                        "model_name": "ButtonStyleModel",
                        "state": {}
                    },
                    "6d2c241530244303bf40c0e3b075e4ce": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "1.2.0",
                        "model_name": "LayoutModel",
                        "state": {}
                    },
                    "876403260ccd4a0a8d22c264a6912605": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "1.2.0",
                        "model_name": "LayoutModel",
                        "state": {}
                    },
                    "91e9ccd185a845428527d57ccb73fb26": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "1.2.0",
                        "model_name": "LayoutModel",
                        "state": {}
                    },
                    "9f021bc8541a4b19862488ae989a54bf": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "1.5.0",
                        "model_name": "ButtonModel",
                        "state": {
                            "description": "Model Accuracy",
                            "layout": "IPY_MODEL_dd9515246a5c4b2c8378751eaa36df79",
                            "style": "IPY_MODEL_04f327f8729c4dfb8064560e564bc0c6"
                        }
                    },
                    "9f1a03a9c53344238bee47b914fa8692": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "1.5.0",
                        "model_name": "ButtonModel",
                        "state": {
                            "description": "Click to see Vocab",
                            "layout": "IPY_MODEL_876403260ccd4a0a8d22c264a6912605",
                            "style": "IPY_MODEL_527b16c6484f41a8bbea8b6f91ee4137"
                        }
                    },
                    "dd9515246a5c4b2c8378751eaa36df79": {
                        "model_module": "@jupyter-widgets/base",
                        "model_module_version": "1.2.0",
                        "model_name": "LayoutModel",
                        "state": {}
                    },
                    "eec603a2a46d48bfbb6570ddefcc6477": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "1.5.0",
                        "model_name": "ButtonModel",
                        "state": {
                            "description": "Model Accuracy",
                            "layout": "IPY_MODEL_91e9ccd185a845428527d57ccb73fb26",
                            "style": "IPY_MODEL_f8efef0d43ab4f32b7ab103f009ffe3b"
                        }
                    },
                    "f7141bc2f4b84b2daf37332f22af3d8f": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "1.5.0",
                        "model_name": "ButtonModel",
                        "state": {
                            "description": "Click to see Vocab",
                            "layout": "IPY_MODEL_6d2c241530244303bf40c0e3b075e4ce",
                            "style": "IPY_MODEL_55400c05e9674a448f4e1fca689799eb"
                        }
                    },
                    "f8efef0d43ab4f32b7ab103f009ffe3b": {
                        "model_module": "@jupyter-widgets/controls",
                        "model_module_version": "1.5.0",
                        "model_name": "ButtonStyleModel",
                        "state": {}
                    }
                },
                "version_major": 2,
                "version_minor": 0
            }
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Text Classification  \n",
                "\n",
                "> Chapter 1 - learn about using a neural network to classify text using sentiment (positive or negative).\n",
                "\n",
                "- toc: true\n",
                "- branch: master\n",
                "- badges: false\n",
                "- comments: false\n",
                "- annotations: true"
            ],
            "metadata": {
                "azdata_cell_guid": "13509f6f-44c1-47f7-98b4-591aa1f1c062"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Introduction"
            ],
            "metadata": {
                "azdata_cell_guid": "51edfc0b-4fb6-4fef-9736-bd551e6e1fc1"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Text classification is the process of assigning tags or categories to text according to its content. It’s one of the fundamental tasks in natural language processing.\n",
                "\n",
                "The text we wanna classify is given as input to an algorithm, the algorithm will then analyze the text’s content, and then categorize the input as one of the tags or categories previously given.\n",
                "\n",
                "**Input → Classifying Algorithm → Classification of Input**\n",
                "\n",
                "Real life examples:\n",
                "\n",
                "- Sentiment analysis: how does the writer of the sentence feel about what they are writing about, do they think positively or negatively of the subject? Ex. restaurant reviews topic labeling: given sentences and a set of topics, which topic does this sentence fall under? Ex. is this essay about history? Math? etc? spam detection Ex. Email filtering: is this email a real important email or spam?\n",
                "\n",
                "Example. A restaurant wants to evaluate their ratings but don’t want to read through all of them. Therefore, they wanna use a computer algorithm to do all their work. They simply want to know if the customer’s review is positive or negative.\n",
                "\n",
                "Here’s an example of a customer’s review and a simple way an algorithm could classify their review.\n",
                "\n",
                "Input: “The food here was too salty and too expensive”\n",
                "\n",
                "Algorithm: Goes through every word in the sentence and counts how many positive words and how many negative words are in the sentence.\n",
                "\n",
                "```\n",
                "    “The, food, here, was, too, and” are all neutral words\n",
                "\n",
                "    “Salty, expensive” are negative words.\n",
                "\n",
                "    Negative words: 2\n",
                "    Positive words: 0\n",
                "```\n",
                "\n",
                "Classification: Negative Review, because there are more negative words (2) than positive (0).\n",
                "\n",
                "However, this algorithm obviously doesn’t work in a lot of cases.\n",
                "\n",
                "For example, “The food here was good, not expensive and not salty” would be classified as negative but it’s actually a positive review.\n",
                "\n",
                "Language and text can get very complicated which makes creating these algorithms difficult. Some things that make language difficult could be words that have multiple meanings, negation words (words such as not), slang, etc."
            ],
            "metadata": {
                "azdata_cell_guid": "66b3dda0-aef7-42e3-adaa-8b0e2772f915"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Set up data and imports"
            ],
            "metadata": {
                "azdata_cell_guid": "e2e811b1-609e-4a92-97fa-c61508274bb4"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Library imports\n",
                "\n",
                "This section of code is to import any necessary Python libraries that we'll need for the rest of this notebook. Some packages may need to be installed since they are not built in to Python3."
            ],
            "metadata": {
                "azdata_cell_guid": "d5e32e42-b3b6-4282-999d-acf05252fb06"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# collapse-hide\n",
                "!pip3 install seaborn\n",
                "!pip3 install plotly --user\n",
                "!pip3 install sklearn\n",
                "\n",
                "import sys\n",
                "import string\n",
                "from scipy import sparse\n",
                "from pprint import pprint\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import plotly.offline as py\n",
                "from plotly.offline import init_notebook_mode, iplot\n",
                "import plotly.graph_objs as go\n",
                "init_notebook_mode(connected = True)\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from html import escape\n",
                "from IPython.core.display import display, HTML\n",
                "from string import Template\n",
                "from sklearn.metrics import classification_report\n",
                "import json\n",
                "\n",
                "HTML('<script src=\"https://d3js.org/d3.v3.min.js\" charset=\"utf-8\"></script>')\n",
                "\n",
                "# Our two files that contain our data, split up into a training set and a testing set.\n",
                "\n",
                "trainingFile = \"dataset/trainingSet.txt\"\n",
                "testingFile = \"dataset/testSet.txt\""
            ],
            "metadata": {
                "azdata_cell_guid": "6305a379-9ec8-4ba3-94e4-e54c9ef30f38"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Requirement already satisfied: seaborn in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.11.1)\n",
                        "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from seaborn) (3.3.3)\n",
                        "Requirement already satisfied: numpy>=1.15 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from seaborn) (1.19.5)\n",
                        "Requirement already satisfied: pandas>=0.23 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from seaborn) (1.2.1)\n",
                        "Requirement already satisfied: scipy>=1.0 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from seaborn) (1.6.0)\n",
                        "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
                        "Requirement already satisfied: cycler>=0.10 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
                        "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\wnsgu\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
                        "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
                        "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.2->seaborn) (8.1.0)\n",
                        "Requirement already satisfied: six in c:\\users\\wnsgu\\appdata\\roaming\\python\\python38\\site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.12.0)\n",
                        "Requirement already satisfied: pytz>=2017.3 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas>=0.23->seaborn) (2020.5)\n",
                        "Requirement already satisfied: plotly in c:\\users\\wnsgu\\appdata\\roaming\\python\\python38\\site-packages (4.14.3)\n",
                        "Requirement already satisfied: six in c:\\users\\wnsgu\\appdata\\roaming\\python\\python38\\site-packages (from plotly) (1.12.0)\n",
                        "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\wnsgu\\appdata\\roaming\\python\\python38\\site-packages (from plotly) (1.3.3)\n",
                        "Requirement already satisfied: sklearn in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.0)\n",
                        "Requirement already satisfied: scikit-learn in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sklearn) (0.24.1)\n",
                        "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.19.5)\n",
                        "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.6.0)\n",
                        "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
                        "Requirement already satisfied: joblib>=0.11 in c:\\users\\wnsgu\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->sklearn) (1.0.0)\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "ImportError",
                    "evalue": "\n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy c-extensions failed.\n- Try uninstalling and reinstalling numpy.\n- If you have already done that, then:\n  1. Check that you expected to use Python3.7 from \"C:\\Users\\wnsgu\\Miniconda3\\python.exe\",\n     and that you have no directories in your PATH or PYTHONPATH that can\n     interfere with the Python and numpy version \"1.18.1\" you're trying to use.\n  2. If (1) looks fine, you can open a new issue at\n     https://github.com/numpy/numpy/issues.  Please include details on:\n     - how you installed Python\n     - how you installed numpy\n     - your operating system\n     - whether or not you have multiple versions of Python installed\n     - if you built from source, your compiler versions and ideally a build log\n\n- If you're working with a numpy git repository, try `git clean -xdf`\n  (removes all files not under version control) and rebuild numpy.\n\nNote: this error has many possible causes, so please don't comment on\nan existing issue about this - open a new one instead.\n\nOriginal error was: DLL load failed: The specified module could not be found.\n",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
                        "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\core\\multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_multiarray_umath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m from numpy.core._multiarray_umath import (\n\u001b[0m\u001b[0;32m      8\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n",
                        "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
                        "\nDuring handling of the above exception, another exception occurred:\n",
                        "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-8-c12376219537>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\scipy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_config\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mshow_numpy_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mshow_numpy_config\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ImportError(\n",
                        "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\numpy\\core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \"\"\" % (sys.version_info[0], sys.version_info[1], sys.executable,\n\u001b[0;32m     53\u001b[0m         __version__, exc)\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0menvkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menv_added\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mImportError\u001b[0m: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy c-extensions failed.\n- Try uninstalling and reinstalling numpy.\n- If you have already done that, then:\n  1. Check that you expected to use Python3.7 from \"C:\\Users\\wnsgu\\Miniconda3\\python.exe\",\n     and that you have no directories in your PATH or PYTHONPATH that can\n     interfere with the Python and numpy version \"1.18.1\" you're trying to use.\n  2. If (1) looks fine, you can open a new issue at\n     https://github.com/numpy/numpy/issues.  Please include details on:\n     - how you installed Python\n     - how you installed numpy\n     - your operating system\n     - whether or not you have multiple versions of Python installed\n     - if you built from source, your compiler versions and ideally a build log\n\n- If you're working with a numpy git repository, try `git clean -xdf`\n  (removes all files not under version control) and rebuild numpy.\n\nNote: this error has many possible causes, so please don't comment on\nan existing issue about this - open a new one instead.\n\nOriginal error was: DLL load failed: The specified module could not be found.\n"
                    ]
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Getting our data \n",
                "\n",
                "Below is a definition of getData, a basic function to pull from the `trainingSet.txt` and `testSet.txt`. The data that we're using for this example is a set of reviews written by users on Yelp, classified as positive (1) or negative (0). \n",
                "\n",
                "We open the file, create temporary arrays, and pull from the file line by line.\n",
                "\n",
                "Open the cell if you'd like to peek into what the function looks like."
            ],
            "metadata": {
                "azdata_cell_guid": "9a1685b0-0396-4ed6-a19f-a43bf2293859"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# collapse-hide\n",
                "\n",
                "def getData(fileName):\n",
                "    f = open(fileName)\n",
                "    file = f.readlines()\n",
                "\n",
                "    sentences = []\n",
                "    sentiments = []\n",
                "\n",
                "    for line in file:\n",
                "        sentence, sentiment = line.split('\\t')\n",
                "        sentences.append(sentence.strip())\n",
                "        sentiments.append(int(sentiment.strip())) # Sentiment in {0,1}\n",
                "\n",
                "    return sentences, np.array(sentiments)"
            ],
            "metadata": {
                "azdata_cell_guid": "73b2409b-c9e0-4fc0-b5c3-93d0d70e9fc5"
            },
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "code",
            "source": [
                "# get data from the training and testing files using the getData function defined above\n",
                "\n",
                "trainingSentences, trainingLabels = getData(trainingFile)\n",
                "testingSentences, testingLabels = getData(testingFile) "
            ],
            "metadata": {
                "azdata_cell_guid": "ea307c06-6f6f-4e2b-9dad-d353d5d63211",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "NameError",
                    "evalue": "name 'trainingFile' is not defined",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-7-a474dbf6f6fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get data from the training and testing files using the getData function defined above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrainingSentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtestingSentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestingLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestingFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'trainingFile' is not defined"
                    ]
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's take a peek at what this data looks like:"
            ],
            "metadata": {
                "azdata_cell_guid": "9859f856-7986-472f-a18b-a176e955b7e8"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#collapse-hide \n",
                "\n",
                "f = open(\"dataset/trainingSet.txt\")\n",
                "file = f.readlines()\n",
                "\n",
                "sentences = []\n",
                "sentiments = []\n",
                "\n",
                "for line in file:\n",
                "    sentence, sentiment = line.split('\\t')\n",
                "    sentences.append(sentence.strip())\n",
                "    sentiments.append(int(sentiment.strip())) \n",
                "    \n",
                "print(\"Sample sentences:\")\n",
                "pprint(sentences[:10]) \n",
                "print(\"Corresponding sentiments:\")\n",
                "pprint(sentiments[:10]) "
            ],
            "metadata": {
                "azdata_cell_guid": "0e6f77c6-9cee-4b4d-a474-9cd47d057bf7",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Sample sentences:\n"
                    ]
                },
                {
                    "output_type": "error",
                    "ename": "NameError",
                    "evalue": "name 'pprint' is not defined",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m<ipython-input-5-76b64ce05817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample sentences:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Corresponding sentiments:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'pprint' is not defined"
                    ]
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Pre-processing our data \n",
                "\n",
                "We need to modify these sentences by tokenizing them into individual strings (word by word) so that we can feed our model individual words and their associated sentiment (negative / positive)."
            ],
            "metadata": {
                "azdata_cell_guid": "38a166de-5063-4638-8c15-19a57e742c68"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def preProcess(sentences):\n",
                "\n",
                "    def cleanText(text):\n",
                "        # Make lower case\n",
                "        text = text.lower()\n",
                "\n",
                "        # Replace non-text characters with spaces\n",
                "        nonText = string.punctuation + (\"\")\n",
                "        text = text.translate(str.maketrans(nonText, ' ' * (len(nonText))))\n",
                "\n",
                "        # Split sentences into individual words - tokenize\n",
                "        words = text.split()\n",
                "\n",
                "        return words\n",
                "\n",
                "    return list(map(cleanText, sentences))"
            ],
            "metadata": {
                "azdata_cell_guid": "a0954a22-336d-469c-a32d-835d7329ba76"
            },
            "outputs": [],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": [
                "# Process both the training and testing tokens.\n",
                "\n",
                "trainingTokens = preProcess(trainingSentences)\n",
                "testingTokens = preProcess(testingSentences)"
            ],
            "metadata": {
                "azdata_cell_guid": "ad995853-5e2e-4ee7-a51f-b1a9a56561e0"
            },
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's look at what these tokenized sentences look like now:"
            ],
            "metadata": {
                "azdata_cell_guid": "55a58566-03ca-4f68-b161-d53318d8b80a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#collapse-hide\n",
                "print(\"Training tokens:\")\n",
                "pprint(trainingTokens[:2]) \n",
                "print(\"Testing tokens:\")\n",
                "pprint(testingTokens[:3]) "
            ],
            "metadata": {
                "azdata_cell_guid": "1e0deff3-61e4-4ad9-b470-e32366be70fb"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Training tokens:\n[['wow', 'loved', 'this', 'place'],\n ['not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty']]\nTesting tokens:\n[['crust', 'is', 'not', 'good'],\n ['would', 'not', 'go', 'back'],\n ['i', 'was', 'shocked', 'because', 'no', 'signs', 'indicate', 'cash', 'only']]\n"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Vectorizing our data\n",
                "\n",
                "Now that we have our sentences tokenized, notice how our training tokens are nested arrays. We want to pull them out of nested arrays and into just one general vocabulary list."
            ],
            "metadata": {
                "azdata_cell_guid": "b88a3dd5-4436-454c-8ce9-3ada65a8f614"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#collapse-hide\n",
                "\n",
                "def getVocab(sentences):\n",
                "    vocab = set()\n",
                "    for sentence in sentences:\n",
                "        for word in sentence:\n",
                "            vocab.add(word)\n",
                "    return sorted(vocab)"
            ],
            "metadata": {
                "azdata_cell_guid": "7536bef9-a077-4690-8efe-a380f9ca0874"
            },
            "outputs": [],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "# Pull trainingTokens into one vocabulary listed, no nested arrays\n",
                "\n",
                "vocabulary = getVocab(trainingTokens)"
            ],
            "metadata": {
                "azdata_cell_guid": "c82b93b2-09b1-466f-881b-5c2730b57388",
                "tags": []
            },
            "outputs": [],
            "execution_count": 9
        },
        {
            "cell_type": "markdown",
            "source": [
                "We can peek at our vocabulary list, an alphabetically sorted list of words, now at a random set of indices:"
            ],
            "metadata": {
                "azdata_cell_guid": "c1fa23a6-2e9d-423f-b7e1-b3fb64d422be"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#collapse-hide\n",
                "\n",
                "pprint(vocabulary[50:70])"
            ],
            "metadata": {
                "azdata_cell_guid": "582421f1-1e8c-4e5b-9bee-a84b742b7a6a",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "['amount',\n 'an',\n 'and',\n 'angry',\n 'another',\n 'anticipated',\n 'any',\n 'anything',\n 'anytime',\n 'anyway',\n 'apologize',\n 'app',\n 'appalling',\n 'appetizers',\n 'apple',\n 'approval',\n 'are',\n 'area',\n 'aren',\n 'aria']\n"
                }
            ],
            "execution_count": 10
        },
        {
            "cell_type": "markdown",
            "source": [
                "We want our arrays to actually be proper vectors to feed to our model, which we'll create below as well. This function, ```createVector``` transforms our arrays into vectors. "
            ],
            "metadata": {
                "azdata_cell_guid": "93e3c874-f1dc-45b8-9c3e-a1cbaf760660"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def createVector(vocab, sentences):\n",
                "    indices = []\n",
                "    wordOccurrences = []\n",
                "\n",
                "    for sentenceIndex, sentence in enumerate(sentences):\n",
                "        alreadyCounted = set() # Keep track of words so we don't double count.\n",
                "        for word in sentence:\n",
                "            if (word in vocab) and word not in alreadyCounted:\n",
                "                # If we just want {0,1} for the presence of the word (bernoulli NB),\n",
                "                # only count each word once. Otherwise (multinomial NB) count each\n",
                "                # occurrence of the word.\n",
                "                \n",
                "            \n",
                "                #which sentence, which word\n",
                "                indices.append((sentenceIndex, vocab.index(word)))\n",
                "                \n",
                "                wordOccurrences.append(1)\n",
                "                alreadyCounted.add(word)\n",
                "\n",
                "    # Unzip\n",
                "    rows = [row for row, _ in indices]\n",
                "    columns = [column for _, column in indices]\n",
                "\n",
                "    sentenceVectors = sparse.csr_matrix((wordOccurrences, (rows, columns)), dtype=int, shape=(len(sentences), len(vocab)))\n",
                "\n",
                "    return sentenceVectors"
            ],
            "metadata": {
                "azdata_cell_guid": "f25af79b-16e4-49b7-acec-18f489f356a6"
            },
            "outputs": [],
            "execution_count": 11
        },
        {
            "cell_type": "code",
            "source": [
                "training = createVector(vocabulary, trainingTokens)\n",
                "testing = createVector(vocabulary, testingTokens)"
            ],
            "metadata": {
                "azdata_cell_guid": "a1683930-c447-4f33-ba91-4a8c3f55ea6e"
            },
            "outputs": [],
            "execution_count": 12
        },
        {
            "cell_type": "markdown",
            "source": [
                "Our training and test data has gone through some transformation. Here's what the training data looks like now:"
            ],
            "metadata": {
                "azdata_cell_guid": "4bbb4757-1f81-4a1c-8341-9879a215fbea"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#collapse-hide\n",
                "\n",
                "print(\"Training data:\")\n",
                "print(training[:2])"
            ],
            "metadata": {
                "azdata_cell_guid": "95527d26-176d-41d6-b90a-3aca0ec7421d"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Training data:\n  (0, 694)\t1\n  (0, 884)\t1\n  (0, 1186)\t1\n  (0, 1335)\t1\n  (1, 52)\t1\n  (1, 640)\t1\n  (1, 768)\t1\n  (1, 788)\t1\n  (1, 1158)\t1\n  (1, 1166)\t1\n  (1, 1171)\t1\n  (1, 1281)\t1\n"
                }
            ],
            "execution_count": 13
        },
        {
            "cell_type": "markdown",
            "source": [
                "# A Naive Bayes model"
            ],
            "metadata": {
                "azdata_cell_guid": "17e17740-7130-4387-93ae-882b87c03e80"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Creating and Training our Model\n",
                "\n",
                "Below is our Naive Bayes classifier, which is the model we've chosen to use for our sentiment analysis of restaurant reviews."
            ],
            "metadata": {
                "azdata_cell_guid": "750df53a-3c17-48ae-a977-7199732afc21"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class NaiveBayesClassifier:\n",
                "    def __init__(self):\n",
                "        self.priorPositive = None  # Probability that a review is positive\n",
                "        self.priorNegative = None  # Probability that a review is negative\n",
                "        self.positiveLogConditionals = None\n",
                "        self.negativeLogConditionals = None\n",
                "\n",
                "    def computePriorProbabilities(self, labels):\n",
                "        self.priorPositive = len([y for y in labels if y == 1]) / len(labels)\n",
                "        self.priorNegative = 1 - self.priorPositive\n",
                "\n",
                "    def computeConditionProbabilities(self, examples, labels, dirichlet=1):\n",
                "        _, vocabularyLength = examples.shape\n",
                "\n",
                "        # How many of each word are there in all of the positive reviews\n",
                "        positiveCounts = np.array([dirichlet for _ in range(vocabularyLength)])\n",
                "        # How many of each word are there in all of the negative reviews\n",
                "        negativeCounts = np.array([dirichlet for _ in range(vocabularyLength)])\n",
                "\n",
                "        # Here's how to iterate through a spare array\n",
                "        coordinates = examples.tocoo()  # Converted to a `coordinate` format\n",
                "        for exampleIndex, featureIndex, observationCount in zip(coordinates.row, coordinates.col, coordinates.data):\n",
                "            # For sentence {exampleIndex}, for word at index {featureIndex}, the word occurred {observationCount} times\n",
                "            if labels[exampleIndex] == 1:\n",
                "                positiveCounts[featureIndex] += observationCount\n",
                "            else:\n",
                "                negativeCounts[featureIndex] += observationCount\n",
                "\n",
                "        # [!] Make sure to use the logs of the probabilities\n",
                "        positiveReviewCount = len([y for y in labels if y == 1])\n",
                "        negativeReviewCount = len([y for y in labels if y == 0])\n",
                "\n",
                "        # We are using bernoulli NB (single occurance of a word)\n",
                "        self.positiveLogConditionals = np.log(positiveCounts) - np.log(positiveReviewCount + dirichlet*2)\n",
                "        self.negativeLogConditionals = np.log(negativeCounts) - np.log(negativeReviewCount + dirichlet*2)\n",
                "\n",
                "        # This works for multinomial NB (multiple occurances of a word)\n",
                "        # self.positiveLogConditionals = np.log(positiveCounts) - np.log(sum(positiveCounts))\n",
                "        # self.negativeLogConditionals = np.log(negativeCounts) - np.log(sum(negativeCounts))\n",
                "\n",
                "    # Calculate all of the parameters for making a naive bayes classification\n",
                "    def fit(self, trainingExamples, trainingLabels):\n",
                "        # Compute the probability of positive/negative review\n",
                "        self.computePriorProbabilities(trainingLabels)\n",
                "\n",
                "        # Compute\n",
                "        self.computeConditionProbabilities(trainingExamples, trainingLabels)\n",
                "\n",
                "    def computeLogPosteriors(self, sentence):\n",
                "        return ((np.log(self.priorPositive) + sum(sentence * self.positiveLogConditionals)),\n",
                "                (np.log(self.priorNegative) + sum(sentence * self.negativeLogConditionals)))\n",
                " \n",
                "    # Have the model try predicting if a review if positive or negative\n",
                "    def predict(self, examples):\n",
                "        totalReviewCount, _ = examples.shape\n",
                "        conf_list = []\n",
                "\n",
                "        predictions = np.array([0 for _ in range(totalReviewCount)])\n",
                "\n",
                "        for index, sentence in enumerate(examples):\n",
                "            logProbabilityPositive, logProbabilityNegative = self.computeLogPosteriors(\n",
                "                sentence)\n",
                "            conf_list.append([np.exp(logProbabilityPositive), np.exp(logProbabilityNegative)])\n",
                "            predictions[index] = 1 if logProbabilityPositive > logProbabilityNegative else 0\n",
                "\n",
                "        return conf_list, predictions"
            ],
            "metadata": {
                "azdata_cell_guid": "17220d70-73b4-4fe6-a5e4-b4ba2dd08e59",
                "tags": []
            },
            "outputs": [],
            "execution_count": 14
        },
        {
            "cell_type": "markdown",
            "source": [
                "Initialize an instance of model and begin to fit the model with our training data and corresponding labels."
            ],
            "metadata": {
                "azdata_cell_guid": "168fc97b-dd2a-46cf-8e79-3934ca77e2b3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "nbClassifier = NaiveBayesClassifier()\n",
                "nbClassifier.fit(training, trainingLabels)"
            ],
            "metadata": {
                "azdata_cell_guid": "65960622-8f88-49d2-aebe-53ffce20d0bb"
            },
            "outputs": [],
            "execution_count": 15
        },
        {
            "cell_type": "code",
            "source": [
                "# determine the accuracy of our model\n",
                "\n",
                "def accuracy(predictions, actual):\n",
                "    return sum((predictions == actual)) / len(actual)"
            ],
            "metadata": {
                "azdata_cell_guid": "15eea9e6-790e-4ada-bd84-26de239c8295"
            },
            "outputs": [],
            "execution_count": 16
        },
        {
            "cell_type": "markdown",
            "source": [
                "Let's take our model for a spin, using both the training set and the testing set. You may notice discrepencies in accuracy between training and testing - _why is that_?"
            ],
            "metadata": {
                "azdata_cell_guid": "53d3f21a-63f5-4f29-b7bf-ae1a888370c9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# run our training and test using the Naive Bayes classifier\n",
                "\n",
                "train_confidence_scores, trainingPredictions = nbClassifier.predict(training)\n",
                "test_confidence_scores, testingPredictions = nbClassifier.predict(testing)"
            ],
            "metadata": {
                "azdata_cell_guid": "48ca6a8b-5d89-44f0-a018-78a890bb1916"
            },
            "outputs": [],
            "execution_count": 17
        },
        {
            "cell_type": "code",
            "source": [
                "#collapse-hide\n",
                "print(\"Training accuracy:\", accuracy(trainingPredictions, trainingLabels))\n",
                "print(\"Testing accuracy:\", accuracy(testingPredictions, testingLabels))"
            ],
            "metadata": {
                "azdata_cell_guid": "4abd7d0b-f009-4fcb-bbcf-92ef339ffa73",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Training accuracy: 0.9519038076152304\nTesting accuracy: 0.7947686116700201\n"
                }
            ],
            "execution_count": 18
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Visualizing Results\n",
                "\n",
                "Here's another to visualize our results using a confusion matrix."
            ],
            "metadata": {
                "azdata_cell_guid": "8ad9abfb-46b3-4a23-a3a7-f13924cf2aac"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#collapse-hide\n",
                "\n",
                "data = {'Actual':    testingLabels,\n",
                "        'Predicted': testingPredictions\n",
                "        }\n",
                "\n",
                "df = pd.DataFrame(data, columns=['Actual','Predicted'])\n",
                "confusion_matrix = pd.crosstab(df['Actual'], df['Predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
                "\n",
                "ax = sns.heatmap(confusion_matrix, annot=True,cmap=\"YlGnBu\")\n",
                "ax.set_ylim(2.0, 0)\n",
                "\n",
                "plt.title('Confusion Matrix of Testing')\n",
                "plt.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "04e2d67d-41a2-47a5-9504-6b8a02cd331a",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 2 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    }
                }
            ],
            "execution_count": 19
        },
        {
            "cell_type": "markdown",
            "source": [
                "## A Closer Look \n",
                "\n",
                "Let's look at the general results for our model - notably, we can look at its precision for predicting negative and positive sentiment in a given sentence."
            ],
            "metadata": {
                "azdata_cell_guid": "03641042-6d20-485f-af4f-cb781092da6a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "target_names = ['negative', 'positive']\n",
                "print(classification_report(testingLabels, testingPredictions, target_names=target_names))"
            ],
            "metadata": {
                "azdata_cell_guid": "a8f1107c-edf7-434e-b4bf-9357b2d39e9e"
            },
            "outputs": [
                {
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-1-73b7949ec6a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestingLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestingPredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"
                    ],
                    "ename": "NameError",
                    "evalue": "name 'classification_report' is not defined",
                    "output_type": "error"
                }
            ],
            "execution_count": 1
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now, we want to make an interactive confusion matrix so we can precisely see which results are accurately classified and which are mis-classified, as well as the confidence at which the model has classified that result. "
            ],
            "metadata": {
                "azdata_cell_guid": "d9da7c80-9581-4910-a39c-233435906799"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# collapse-hide\n",
                "\n",
                "# work with the model results to create a JSON dump of the data for future use\n",
                "\n",
                "import json\n",
                "\n",
                "output_filename = \"predict_naive.json\"\n",
                "data = []\n",
                "for i in range(len(testingPredictions)):\n",
                "  data.append({\n",
                "      'index': i,\n",
                "      'true_label': int(testingLabels[i]),\n",
                "      'predicted_label': int(testingPredictions[i]),\n",
                "      'confidence_score': test_confidence_scores[i],\n",
                "      'text': testingSentences[i]\n",
                "  })\n",
                "\n",
                "with open(output_filename, 'w') as outfile:\n",
                "    json.dump(data, outfile, indent=4, sort_keys=False)"
            ],
            "metadata": {
                "azdata_cell_guid": "8341b403-049a-4deb-935a-0007ffb43f90",
                "tags": []
            },
            "outputs": [],
            "execution_count": 37
        },
        {
            "cell_type": "code",
            "source": [
                "# collapse-hide\n",
                "\n",
                "from IPython.core.display import display, HTML\n",
                "from string import Template\n",
                "\n",
                "\n",
                "json_filepath = \"\\\"\" + output_filename + \"\\\"\"\n",
                "HTML('<script src=\"https://d3js.org/d3.v3.min.js\" charset=\"utf-8\"></script>')"
            ],
            "metadata": {
                "azdata_cell_guid": "95038aa7-dc75-4a32-a550-bb4c05f2e0a7"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 38,
                    "data": {
                        "text/html": "<script src=\"https://d3js.org/d3.v3.min.js\" charset=\"utf-8\"></script>",
                        "text/plain": "<IPython.core.display.HTML object>"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 38
        }
    ]
}